{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee994faf",
   "metadata": {},
   "source": [
    "Building an AI Image Caption Recommendation System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9d6ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dependecies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da95248d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import numpy as np\n",
    "from typing import List, Tuple\n",
    "import sys\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c806d09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.dirname(os.path.abspath('.')))\n",
    "from caption_recommendation import ImageCaptionRecommendationSystem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9532e6f",
   "metadata": {},
   "source": [
    "function for handling the loading and preprocessing of an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c412497c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_image(image_path: str) -> Tuple[dict, CLIPProcessor]:\n",
    "    \"\"\"Load and preprocess an image for CLIP model.\"\"\"\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    inputs = processor(images=image, return_tensors=\"pt\")\n",
    "    return inputs, processor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cf8515",
   "metadata": {},
   "source": [
    "Generating Image Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ff49ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_image_embeddings(inputs: dict) -> Tuple[torch.Tensor, CLIPModel]:\n",
    "    \"\"\"Generate image embeddings using CLIP model.\"\"\"\n",
    "    model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        image_features = model.get_image_features(**inputs)\n",
    "    return image_features, model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27e4d5c",
   "metadata": {},
   "source": [
    "function to match captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e340844",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def rank_captions(\n",
    "    image_features: torch.Tensor, \n",
    "    captions: List[str], \n",
    "    model: CLIPModel, \n",
    "    processor: CLIPProcessor\n",
    ") -> Tuple[List[str], List[float]]:\n",
    "    \"\"\"Rank captions by similarity to image features.\"\"\"\n",
    "    # Process text inputs\n",
    "    text_inputs = processor(\n",
    "        text=captions, \n",
    "        return_tensors=\"pt\", \n",
    "        padding=True, \n",
    "        truncation=True\n",
    "    )\n",
    "    \n",
    "    # Generate text features\n",
    "    with torch.no_grad():\n",
    "        text_features = model.get_text_features(**text_inputs)\n",
    "    \n",
    "    # Normalize features\n",
    "    image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "    \n",
    "    # Calculate similarity scores\n",
    "    similarity_scores = torch.matmul(image_features, text_features.T).squeeze(0)\n",
    "    \n",
    "    # Sort captions by similarity score (descending)\n",
    "    sorted_indices = torch.argsort(similarity_scores, descending=True)\n",
    "    sorted_captions = [captions[i] for i in sorted_indices]\n",
    "    sorted_scores = similarity_scores[sorted_indices].tolist()\n",
    "    \n",
    "    return sorted_captions, sorted_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835351fa",
   "metadata": {},
   "source": [
    "driver function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93292c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_top_captions(\n",
    "    image_path: str, \n",
    "    candidate_captions: List[str], \n",
    "    top_n: int = 5\n",
    ") -> Tuple[List[str], List[float]]:\n",
    "    \"\"\"Get top-n captions for an image from candidate list.\"\"\"\n",
    "    recommendations = st.session_state.model.rank_predefined_captions(\n",
    "        image_path=image_path,\n",
    "        candidate_captions=candidate_captions,\n",
    "        top_n=top_n\n",
    "    )\n",
    "    \n",
    "    # Extract captions and scores\n",
    "    captions = [rec[0] for rec in recommendations]\n",
    "    scores = [rec[1] for rec in recommendations]\n",
    "    \n",
    "    return captions, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c6f840",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_and_rank_captions(\n",
    "    image_path: str, \n",
    "    keywords: List[str], \n",
    "    top_n: int = 5, \n",
    "    num_candidates: int = 10\n",
    ") -> Tuple[List[str], List[float]]:\n",
    "    \"\"\"Generate captions from keywords and rank them by image similarity.\"\"\"\n",
    "    # Initialize the recommendation system\n",
    "    recommender = ImageCaptionRecommendationSystem()\n",
    "    \n",
    "    # Generate candidate captions using keywords\n",
    "    candidate_captions = recommender.generate_captions(keywords, num_candidates)\n",
    "    \n",
    "    if not candidate_captions:\n",
    "        return [], []\n",
    "    \n",
    "    # Load and preprocess image\n",
    "    inputs, processor = load_and_preprocess_image(image_path)\n",
    "    \n",
    "    # Generate image embeddings\n",
    "    image_features, model = generate_image_embeddings(inputs)\n",
    "    \n",
    "    # Rank captions by similarity\n",
    "    sorted_captions, sorted_scores = rank_captions(\n",
    "        image_features, candidate_captions, model, processor\n",
    "    )\n",
    "    \n",
    "    # Return top-n captions and scores\n",
    "    top_n = min(top_n, len(sorted_captions))\n",
    "    return sorted_captions[:top_n], sorted_scores[:top_n]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba05285b",
   "metadata": {},
   "source": [
    "Bellow is a alternative way"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c3280d",
   "metadata": {},
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "best_match_captios,similarities=image_captioning(\"path\",candidate_captions)\n",
    "top-n=min(5,len(best_mtch_caption))\n",
    "top_best_caprtions=best_captions[:top_n]\n",
    "top_similarities=similarities[:top_n]   \n",
    "PRINT(\"Most suitable captions\")\n",
    "for i,(caption,similarity) in enumerate(zip(top_best_caprtions,top_similarities)):\n",
    "    print(f\"{i+1}.{caption} (Similarity:{similarity:.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0283c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
